{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import urllib.request # Download images\n",
    "from PIL import *   # Read image\n",
    "from langdetect import detect # Books only in english\n",
    "\n",
    "import git # To clone and download the csv files\n",
    "\n",
    "from bs4 import BeautifulSoup # For web scraping\n",
    "import requests # For web scraping\n",
    "import json # For web scraping\n",
    "import re # Regex\n",
    "\n",
    "from time import time \n",
    "import shutil # To remove folders\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from myfunk import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/iZbra1/Documents/K2DS/Projects/book_cover_recommender\n"
     ]
    }
   ],
   "source": [
    "## ---- Variables\n",
    "# Check Path\n",
    "\n",
    "project_name = 'book_cover_recommender'\n",
    "\n",
    "if not os.getcwd().endswith(project_name):\n",
    "    try: \n",
    "        ind = os.getcwd().index(project_name)\n",
    "        idx = len(os.getcwd()) - (ind+len(project_name))\n",
    "        path = os.getcwd()[:-idx]\n",
    "        print(path)\n",
    "        os.chdir(path)\n",
    "\n",
    "    except:\n",
    "        print('{} not found in path'.format(project_name))\n",
    "else: \n",
    "    path = os.getcwd()\n",
    "    print(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files are already in place (10000, 23)\n"
     ]
    }
   ],
   "source": [
    "## ----  Web scraping:  ~ 1 min\n",
    "\n",
    "try: \n",
    "    # See if the 5 csv files are already in the external folder\n",
    "    \n",
    "    assert len([f for f in glob.glob(path+\"/data/external/*.csv\")]) == 5\n",
    "    print('CSV files are already in place', pd.read_csv(path+\"/data/external/books.csv\").shape)\n",
    "\n",
    "    \n",
    "except:\n",
    "    # Get the data from goodreads 10k\n",
    "    # https://github.com/zygmuntz/goodbooks-10k.git\n",
    "    # save the csv files in ~/data/external\n",
    "    print('Getting files from the github repo...')\n",
    "    start = time()\n",
    "    download_data_from_goodbooks(path)\n",
    "    print(time()-start,'seconds processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_goodbooks(path):\n",
    "    '''Preps and cleans the external csv files:\n",
    "        - Selects the relevant columns\n",
    "        - Remove books that dont have ISBN\n",
    "        - Unify the language_code\n",
    "        - Correct the missing language_code\n",
    "        - Get the index with english titles only\n",
    "        - Drop the language_code column\n",
    "        - Assign a logical label name to the current feature\n",
    "        - Checks the validity of data in feature: years\n",
    "        - Casting to adequate types\n",
    "        - Set authors and title to lower case and remove punctuation\n",
    "        - Saves csv preprocessed files on the raw subfolder\n",
    "    '''\n",
    "    ## ---- books.csv\n",
    "    # Load csv file from goodbooks-10k\n",
    "    cols = ['book_id','isbn', 'authors', 'original_publication_year',\n",
    "           'title', 'language_code', 'image_url', 'average_rating', 'ratings_count']\n",
    "    df = pd.read_csv(path+'/data/external/books.csv', usecols=cols, index_col=0)\n",
    "    print('Preprocessing {} books'.format(df.shape[0]))\n",
    "\n",
    "    # Remove books that dont have ISBN\n",
    "    df.dropna(subset=['isbn'], inplace=True)\n",
    "\n",
    "    # Unify the language_code\n",
    "    df.loc[df.language_code.str.startswith('en', na=False),'language_code'] = 'en'\n",
    "    print('Found {} books officially in english.\\nRunning the english detector for the rest'.\\\n",
    "    format(df.loc[df.language_code=='en'].shape[0]))\n",
    "\n",
    "    # Correct the missing language_code:\n",
    "    df_lang = df.loc[df.language_code.isnull()].title.apply(lambda x: detect(x))\n",
    "    df.loc[df_lang.index,'language_code'] = df_lang\n",
    "    print('Detector found {} more books in english'.format((df_lang=='en').sum()))\n",
    "\n",
    "    # Get the index with english titles only\n",
    "    df = df.loc[df.language_code=='en']\n",
    "\n",
    "    # Drop the language_code column\n",
    "    df.drop('language_code', axis=1, inplace=True)\n",
    "\n",
    "    # Rename column original_publication_year to year\n",
    "    df.rename(columns={'original_publication_year': 'year'}, inplace=True)\n",
    "\n",
    "    # Set invalid years to 0\n",
    "    df.loc[df.year<0,'year'] = 0\n",
    "\n",
    "    # Set Nan's to 0\n",
    "    df['year'] = df['year'].fillna(0)\n",
    "\n",
    "    # Convert to int\n",
    "    df.year = df.year.astype(int)\n",
    "\n",
    "\n",
    "    # Set authors and title to lower case and remove punctuation\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "    df['authors'] = df.authors.apply(lambda x: re.sub(punctuation_pattern, '', unidecode.unidecode(x).lower().strip()))\n",
    "    df['title'] = df.title.apply(lambda x: re.sub(punctuation_pattern, '', unidecode.unidecode(x).lower().strip()))\n",
    "\n",
    "    # Save file (will be saved as UTF-8 by default!)\n",
    "    df.to_csv(path+'/data/raw/books.csv')\n",
    "    print(\"Preprocessed\", df.shape[0], 'books')\n",
    "\n",
    "    # Get the book id's that remain\n",
    "    books_available = df.index.values\n",
    "\n",
    "\n",
    "## ---- book_tags.csv\n",
    "    df = pd.read_csv(path+'/data/external/book_tags.csv')\n",
    "    df.rename(columns={'goodreads_book_id':'book_id'}, inplace=True)\n",
    "    df = df.loc[df.book_id.isin(books_available)]\n",
    "\n",
    "    # Save file\n",
    "    df.to_csv(path+'/data/raw/book_tags.csv')\n",
    "    print(\"Preprocessed\", df.shape[0], 'book_tags')\n",
    "\n",
    "## --- tags.csv\n",
    "    copyfile(path+'/data/external/tags.csv', path+'/data/raw/tags.csv')\n",
    "\n",
    "## --- ratings.csv\n",
    "    df = pd.read_csv(path + '/data/external/ratings.csv')\n",
    "    df = df.loc[df.book_id.isin(books_available)]\n",
    "    # Save file\n",
    "    df.to_csv(path + '/data/raw/ratings.csv')\n",
    "\n",
    "## --- to_read.csv\n",
    "    df = pd.read_csv(path + '/data/external/to_read.csv')\n",
    "    df = df.loc[df.book_id.isin(books_available)]\n",
    "\n",
    "    # Save file\n",
    "    df.to_csv(path+'/data/raw/to_read.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing 10000 books\n",
      "Found 8142 books officially in english.\n",
      "Running the english detector for the rest\n",
      "Detector found 870 more books in english\n",
      "Preprocessed 9012 books\n",
      "Preprocessed 73000 book_tags\n",
      "56.8722608089447 seconds processed\n"
     ]
    }
   ],
   "source": [
    "## --- Preprocessing: ~ 20 s\n",
    "# Preprocess the csv files from the ~data/external folder \n",
    "# and save them in ~data/raw folder\n",
    "\n",
    "start = time()\n",
    "preprocess_goodbooks(path)\n",
    "print(time()-start,'seconds processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----- Upload the csv files to the sql instance\n",
    "\n",
    "# df = pd.read_csv(path+'data/interim/books.csv', index_col=0)\n",
    "# insert_db(df, 'books', engine)\n",
    "# print('Uploaded: books')\n",
    "# df2 = pd.read_csv(path+'data/external/ratings.csv', index_col=0)\n",
    "# insert_db(df2, 'ratings', engine)\n",
    "# print('Uploaded: ratings')\n",
    "# df3 = pd.read_csv(path+'data/external/to_read.csv', index_col=0)\n",
    "# insert_db(df3, 'to_read', engine)\n",
    "# print('Uploaded: to_read')\n",
    "# df4 = pd.read_csv(path+'data/external/tags.csv', index_col=0)\n",
    "# insert_db(df4, 'tags', engine)\n",
    "# print('Uploaded: tags')\n",
    "# df5 = pd.read_csv(path+'data/external/book_tags.csv', index_col=0)\n",
    "# insert_db(df5, 'book_tags', engine)\n",
    "# print('Uploaded: book tags')\n",
    "# print('insert_db done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9012, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>61711527</td>\n",
       "      <td>peggy orenstein</td>\n",
       "      <td>2011</td>\n",
       "      <td>cinderella ate my daughter dispatches from the...</td>\n",
       "      <td>3.65</td>\n",
       "      <td>11279</td>\n",
       "      <td>https://images.gr-assets.com/books/1279214118m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>375700455</td>\n",
       "      <td>john keegan</td>\n",
       "      <td>1998</td>\n",
       "      <td>the first world war</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9162</td>\n",
       "      <td>https://images.gr-assets.com/books/1403194704m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              isbn          authors  year  \\\n",
       "book_id                                     \n",
       "9999      61711527  peggy orenstein  2011   \n",
       "10000    375700455      john keegan  1998   \n",
       "\n",
       "                                                     title  average_rating  \\\n",
       "book_id                                                                      \n",
       "9999     cinderella ate my daughter dispatches from the...            3.65   \n",
       "10000                                  the first world war            4.00   \n",
       "\n",
       "         ratings_count                                          image_url  \n",
       "book_id                                                                    \n",
       "9999             11279  https://images.gr-assets.com/books/1279214118m...  \n",
       "10000             9162  https://images.gr-assets.com/books/1403194704m...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path+'/data/raw/books.csv', index_col=0)\n",
    "print(df.shape)\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check book covers or download them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "# Function that formats image filenames into the standard format\n",
    "format_cover_file = lambda x: re.sub(punctuation_pattern, '', unidecode.unidecode(x).lower().strip())\n",
    "\n",
    "# Function that returns True if the image file exists. \n",
    "sync = lambda x: os.path.isfile(os.getcwd()+'/'+x.title+\" by \"+x.authors+\".jpg\")\n",
    "\n",
    "# Download Book Covers from image_url\n",
    "def get_cover(row):\n",
    "    filename = row.title+' by '+row.authors+'.jpg'\n",
    "    try: \n",
    "        urllib.request.urlretrieve(row.image_url,filename)\n",
    "    except: \n",
    "        print('image url not found: ', row.image_url)\n",
    "        filename = 'Img not found'\n",
    "    return filename\n",
    "\n",
    "def format_image_filenames(cover_titles):\n",
    "    for file in cover_titles:\n",
    "    # Check if file exists\n",
    "        if os.path.isfile(file+\".jpg\"):\n",
    "#             print('Renaming..',file+\".jpg\")\n",
    "            os.rename(file+\".jpg\", format_cover_file(file)+\".jpg\")\n",
    "        else:\n",
    "            print(file+\".jpg doesnt exist/n\")\n",
    "\n",
    "def check_cover(x, df, pth, path):\n",
    "    try: \n",
    "        os.chdir(pth)\n",
    "        # Check if all covers have a reference in the dataframe\n",
    "        cover_titles = [f[:-4] for f in glob.glob(\"*.jpg\")]  \n",
    "        number_of_covers = len(cover_titles)\n",
    "        print('{} covers found'.format(number_of_covers))\n",
    "        if number_of_covers == 0:\n",
    "            return 0\n",
    "        number_covers_reference = df.shape[0]\n",
    "        # Apply format to cover files:  \n",
    "        print('Checking filename Format')\n",
    "        format_image_filenames(cover_titles)\n",
    "    \n",
    "    except Exception as e: \n",
    "        print('No covers found in {} drive:\\n{}'.format(x,e))\n",
    "        return 0 #'none'\n",
    "\n",
    "    if number_of_covers > 0 and not number_of_covers == number_covers_reference:\n",
    "        print('There are {} out of {} book covers that couldnt be downloaded'.\\\n",
    "              format(number_covers_reference-number_of_covers, number_covers_reference))\n",
    "        # Remove from the dataset\n",
    "        df['cover_exists'] = df.apply(sync, axis=1)\n",
    "        print('Removing {} instances from the dataset'.format(number_covers_reference-number_of_covers))\n",
    "        df = df[df.cover_exists]\n",
    "        # Column holding filename\n",
    "        df['cover'] = df.apply(lambda x: x.title+' by '+x.authors+\".jpg\", axis=1)\n",
    "\n",
    "        # Sanity check\n",
    "        # Remove cover files that dont have a reference in the dataframe. \n",
    "        img_files = [f[:-4] for f in glob.glob(\"*.jpg\")] \n",
    "        df['filename'] = df.cover.apply(lambda x: x[:-4])\n",
    "        if df.shape[0] < len(img_files):\n",
    "\n",
    "            book_list = df.filename.values\n",
    "            for img in img_files:\n",
    "                if img not in book_list:\n",
    "                    os.remove(img+'.jpg')\n",
    "        img_files = [f[:-4] for f in glob.glob(\"*.jpg\")]       \n",
    "        if len(img_files) == df.shape[0]:\n",
    "            print(\"Database consolidated\")\n",
    "        else: \n",
    "            print(\"Something went while in consolidating database: there are \", df.shape[0], \" books in the dataframe and \", len(img_files), \" cover images.\")\n",
    "        # Move covers folder to raw\n",
    "        print(\"Copying images to data/raw/covers\")\n",
    "        src = os.getcwd()\n",
    "        dst = path+\"/data/raw/covers\"\n",
    "\n",
    "        try:\n",
    "            #if path already exists, remove it before copying with copytree()\n",
    "            if os.path.exists(dst):\n",
    "                shutil.rmtree(dst)\n",
    "            else: \n",
    "                os.mkdir(dst)\n",
    "                \n",
    "            shutil.copytree(src, dst)\n",
    "            print('Image files copied!')\n",
    "        except OSError as e:\n",
    "            # If the error was caused because the source wasn't a directory\n",
    "            if e.errno == errno.ENOTDIR:\n",
    "                shutil.copy(source_dir_prompt, destination_dir_prompt)\n",
    "            else:\n",
    "                print('Directory not copied. Error: %s' % e)\n",
    "        \n",
    "        return df\n",
    "    else: \n",
    "        print(\"Dataset is complete!\",df.shape, number_of_covers)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select Drive: [external/local/none]\n",
      "8988 covers found\n",
      "Checking filename Format\n",
      "There are 24 out of 9012 book covers that couldnt be downloaded\n",
      "Removing 24 instances from the dataset\n",
      "Database consolidated\n",
      "Copying images to data/raw/covers\n",
      "Path does exist!, removing\n",
      "Image files copied!\n"
     ]
    }
   ],
   "source": [
    "print(\"Select Drive: [external/local/none]\")\n",
    "x = input()\n",
    "if x == 'external':\n",
    "    x = check_cover(x, df, '/Volumes/LEEGARE/Capstone', path)\n",
    "\n",
    "    \n",
    "elif x == 'local':\n",
    "    # Local drive\n",
    "    try: \n",
    "        os.mkdir(path+'/data/external/covers')\n",
    "    except: \n",
    "        pass\n",
    "    x = check_cover(x, df, path+\"/data/external/covers/Capstone\", path)\n",
    "\n",
    "\n",
    "if not isinstance(x,pd.core.frame.DataFrame):\n",
    "    print('Downloading book covers... into {} ETA: 68 min for 9011 books | 45s for 100 books'.format(path))\n",
    "# if os.getcwd()\n",
    "    start = time() # 68 min for 9011 books | 45s for 100 books\n",
    "    df['cover'] = df.apply(lambda row: get_cover(row), axis=1)\n",
    "    print(time()-start, 's downloading', df.shape[0], 'book covers')\n",
    "    print(\"Covers saved in \", path+\"/data/external/covers\")\n",
    "else:\n",
    "    df = x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_books = df.reset_index().book_id.unique()\n",
    "assert len(av_books) == len([f[:-4] for f in glob.glob(path+\"/data/raw/covers/*.jpg\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books saved\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "df.drop([\"cover_exists\",\"filename\"], axis=1, inplace=True)\n",
    "df.to_csv(path+'/data/raw/books.csv')\n",
    "print(\"Books saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8988, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings saved\n"
     ]
    }
   ],
   "source": [
    "# Get the RATINGS of the sample books: \n",
    "rat = pd.read_csv(path+\"/data/raw/ratings.csv\", index_col=0)\n",
    "rat = rat.loc[rat.book_id.isin(av_books)]\n",
    "av_users = rat.reset_index().user_id.unique()\n",
    "rat.to_csv(path+'/data/raw/ratings.csv')\n",
    "print(\"Ratings saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books to read saved!\n",
      "(861087, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>7173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  book_id\n",
       "0        9        8\n",
       "1       15      398\n",
       "2       15      275\n",
       "3       37     7173\n",
       "4       34      380"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the USERS in RATINGS that want to read the BOOKS available\n",
    "df_read = pd.read_csv(path+\"/data/raw/to_read.csv\", index_col=0)\n",
    "df_read = df_read.loc[(df_read.user_id.isin(av_users))&(df_read.book_id.isin(av_books))]\n",
    "df_read.to_csv(path+'/data/raw/to_read.csv')\n",
    "print('Books to read saved!',df_read.shape,sep='\\n')\n",
    "df_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
